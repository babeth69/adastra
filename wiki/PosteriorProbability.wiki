#summary Need for posterior probability in ML

= Introduction =

For some practical usage posterior probability is equally important as the actual Machine Learning classification. We need to know what are the chances that this is the right classification. 

If you control a wheel chair for example with left, right and forward commands then you need to say how much left, how much right or forward you need to go. Another necessity comes from the fact that classificator that has only three classes will always return result. It you had the posterior probabilities then it would of been easy to simply put thresh-hold that classification with probability under 0.2 should be ignored and this will help the wheelchair do not receive hundreds of commands for different directions for a short period of time. 

Logisitic Regression provides the needed posterior probability we could say by design thanks to the sigmoid function. By other such as SVM do not. You could use the result from the voting during the multi-class classification, but this is definitely suboptimal and discrete.     


